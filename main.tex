\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
%\documentclass{svjour3}                     % onecolumn (standard format)
% \documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
% \documentclass[smallextended]{svjour3}       % onecolumn (second format)
\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
\usepackage[utf8]{inputenc}
\usepackage[hyphens,spaces,obeyspaces]{url}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{physics}
\usepackage{natbib}
\usepackage{tikz}
\usetikzlibrary{quantikz}
\usepackage{graphicx}
\usepackage[font={small}]{caption}
\usepackage{subcaption}
\graphicspath{{figs/}}
\usepackage{todonotes}
\newcommand{\ma}[1]{\todo[color=red!40,inline]{M: #1}}
\newcommand{\eli}[1]{\todo[color=yellow!40,inline]{E: #1}}
\begin{document}

\title{Quantum Algorithm for Principal Component Analysis}
\subtitle{Quantum Algorithm for Principal Component Analysis}
\author{Riccardo Santambrogio\and Michele Scandelli\and Daniele Tagliabue\and Mehrnoosh Askarpour \and Elisabetta Di~Nitto}
\institute{Riccardo Santambrogio, Michele Scandelli, Daniele Tagliabue \at
              DEIB, Politecnico di Milano, Italy \\
%             %   Tel.: +123-45-678910\\
%             %   Fax: +123-45-678910\\
              \email{\{name.surname\}@mail.polimi.it}       
% %             \emph{Present address:} of F. Author  %  if needed
          \and
          Mehrnoosh Askarpour \at
              Department of Computing and Software, McMaster University, Canada
              \and 
              Elisabetta Di~Nitto\at
              DEIB, Politecnico di Milano, Italy
}
%
\date{Received: date / Accepted: date}
%
\maketitle
\begin{abstract}
   As new milestones are reached in quantum computing and the technology advances towards maturity, an important and promising field of application is that of Quantum Machine Learning.
   Principal Component Analysis is one example of a problem that can benefit from the intrinsic strength of quantum computation at performing linear algebra.
   Following the original proposal of a Quantum PCA (QPCA) algorithm, various efforts have been made in the literature to design a procedure applicable on small or intermediate-scale quantum computers.
   We follow this direction and propose an implementation of the algorithm that can be executed on currently available quantum computers to reconstruct the principal subspace of a matrix.
   We provide Qiskit code for this implementation and show its application on both the simulator and IBM's real quantum processor.
\end{abstract}

\section{Introduction}
Principal Component Analysis (PCA) is a mathematical technique that is widely used in machine learning for applications such as dimensionality reduction and lossy data compression.\ma{add references to these two applications}
A common interpretation of PCA is that of orthogonal projection of a dataset onto a lower dimensional linear space (the \textit{principal subspace}) such that the variance of the projected data is maximized \cite{10.5555/1162264}. Given a $d$ dimensional dataset, the algorithm builds $d$ new features as linear combinations of the original ones and ranks them by the portion of variance in the data that they capture. By retaining the top $k$ of these new features, one obtains the lower dimensional space that is optimal in terms of explained variance. This is done by working on the covariance matrix of the standardized data. The eigenvectors of this matrix are the new candidate features and the corresponding eigenvalues are proportional to the amount of captured variance. If the covariance matrix was normalized with respect to its trace, then the eigenvalues express directly a percentage. 
Hence, performing eigendecomposition of the covariance matrix is enough to determine the principal subspace. This technique proves especially useful when only a small number of principal components can explain most of the variance in the data, i.e., when many eigenvalues are close to zero or small, as this allows for greater compression with low reconstruction error.
The most expensive steps of the algorithm are the computation of the covariance matrix and the eigendecomposition. We only address the latter, and show how to use quantum methods proposed in the literature to find eigenvalues and eigenvectors of a real-valued positive semi-definite matrix, such as a covariance matrix.

\subsection*{Contributions}
In this work we explore the possibilities of a general algorithm for Quantum PCA (QPCA) that uses currently available quantum frameworks, with reference especially to Qiskit and the IBM Q devices.
We build our approach on the methods proposed in previous works, surveyed in Section~\ref{sec:rw}, and in particular on the procedure of \cite{martin2019pricing}.

We consider the case in which the basis vectors are to be decoded into a classical description, and describe a method that through consecutive measurements allows to reconstruct the principal subspace of a generic covariance matrix that could be given in different forms (depending on how the unitaries are constructed, as discussed later), when starting from a random mixed state.

We extend the approach demonstrated in \cite{martin2019pricing} to the case in which multiple dominant eigenvalues and eigenvectors are to be identified, to the point of reconstructing in principle the original matrix if it is not approximately low-rank.

We provide Qiskit code for this implementation that can be executed on IBM's real quantum computers, and we test said implementation on the simulator and on a real quantum device to see how it behaves and what problems can arise in practice.

The rest of this paper is structured as follows: Section~\ref{sec:rw} surveys related works; Section~\ref{sec:pre} presents preliminaries for the work; Section~\ref{sec:impl} explains the specific approach that was adopted and describes our implementation; Section~\ref{sec:exp} presents experiments with examples on the simulator and tests on the real quantum device; Section~\ref{sec:disc} contains discussions, further analysis of our implementation and comments on the experiments; Section~\ref{sec:conc} contains conclusions.



\section{Related Works}
\label{sec:rw}
An algorithm for QPCA was first introduced by \cite{Lloyd_2014}, where a self-tomography method is proposed that, given multiple copies of a quantum state with density matrix $\rho$, produces a state dominated by the principal components of $\rho$.
When applied to a state encoding the covariance matrix of a dataset, this procedure allows to have quantum access to the directions of greatest variance of the data.

Following this, \cite{yu2018quantum} address the problem of completing the desired compression task by performing directly the mapping of a dataset into the low-dimensional space. 
In their algorithm, they assume to have quantum access to the dataset stored in a QRAM and apply QPCA to produce a state encoding the low-dimensional representation of the data, so that it can then be used to implement other quantum algorithms using less resources.

On the long term, the methods in~\cite{Lloyd_2014} and~\cite{yu2018quantum} represent an important preprocessing tool in the framework of Quantum Machine Learning (QML) as they enable execution of many machine learning tasks on quantum data. Currently, however, their implementation in the original formulation presents some difficulties.
In particular, until large, practical QRAM implementations are available, procedures to prepare a state encoding the covariance matrix of a quantumly accessible dataset seem hardly feasible.
Nevertheless, subsequent works have shown that the algorithms can be adapted to work with any initial state, as long as the matrix can be encoded in the unitary operators.

\cite{Daskin_2016} proposes a variant of the QPCA algorithm that works, in principle, with random initial states --although authors specifically use the equal superposition in the computational basis-- and applies amplitude amplification to mark and increase the amplitude of eigenvalues lying in a pre-defined range.
In this way, a linear combination of the eigenvectors associated to the desired largest eigenvalues can be obtained, encoded in the output state.
However as the authors explain, in order for this method to succeed, some way must be found to estimate the right number of applications of the amplitude amplification routine, as this determines an oscillation of the success probability.

A different approach is taken by ~\cite{martin2019pricing}, which propose an iterative method that can be applied using a random state to estimate the largest principal component of a covariance matrix that is not provided in quantum form.
Starting from an assumption that the input matrix is well approximated by a matrix with lower rank $r$ and projecting the eigenvalue component on the state corresponding to $1/r$, authors first execute the circuit using a random initial state and then sequentially improve the result by repeating the procedure using the estimated largest eigenvector as new initial state. 
They apply this algorithm to a practical scenario in finance, and show that for small instances it is possible to reconstruct the first principal component from a set of data on a real IBM quantum processor.

\section{Preliminaries}
\label{sec:pre}
The main idea of the QPCA algorithm, common to its different variants, consists in applying Quantum Phase Estimation (QPE) to reveal information about eigenvalues and eigenvectors of the matrix.

QPE is a core technique used in many quantum algorithms to estimate the phase of an eigenvector of a unitary operator, i.e., given a unitary operator $U$ and eigenstate $\ket{\psi}$ such that $U\ket{\psi} = e^{2\pi i\varphi}\ket{\psi}$, the QPE estimates $\varphi$. 
An explanation of this well-known technique, motivated by an application in quantum chemistry, is provided in \cite{10.5555/1481129}.
In order to apply this to a covariance matrix $X$, we consider $U=e^{iX}$. $X$ is normalized w.r.t. its trace, so that $U$ is unitary, and ${2\pi\varphi}$ is eigenvalue of $X$ corresponding to eigenvector $\ket{\psi}$.


The quantum circuit for the QPE uses first Phase Kickback to encode $\varphi$ into an $m$ qubits register, in the form below, which is called \textit{phase state}.
\begin{equation}
\label{eqn:phasestate}
\ket{\Phi} = \frac{1}{\sqrt{2^m}} \sum_{y=0}^{2^m-1}e^{2\pi i\varphi y}\ket{y}
\end{equation} 
This step requires that $e^{iX}$ be implemented as a (controlled) quantum gate; this nontrivial operation is discussed later but for now we assume it can be done efficiently.\\
After the phase state of Eq. (\ref{eqn:phasestate}) has been synthesized, $\varphi$ is obtained by applying the Inverse Quantum Fourier Transform (IQFT) to it and then measuring the result in the computational basis. The $m$ qubits of the original register collapse with high probability to binary values that reveal the binary fraction representation of $\varphi$. Since $\varphi$ is in general a real number, for irrational values the procedure yields an approximate answer over $m$ bits. 
\begin{equation}
\label{eqn:binexpansion}
\varphi \approx 0.x_{1}x_{2}...x_{m-1}x_{m}
\end{equation}
The probability of measuring the best $m$-bit approximation of $\varphi$ can be increased to $1-\varepsilon$ by increasing the number of qubits by $\mathcal{O}(\log{(1/\varepsilon)})$ and then rounding off the result to the most significant $m$ bits, as described in \cite{Cleve_1998}.

The complete circuit that implements the QPE is shown in Fig.\ref{fig:qpecirc}.
While in many applications this algorithm is used to find the eigenvalue corresponding to a known eigenvector, we need to perform eigendecomposition having no prior knowledge of any of the eigenvectors of $X$, and therefore we cannot prepare the state $\ket{\psi}$. The intuition that allows to still make use of this, provided by \cite{Abrams_1999}, is that, labelling the eigenvectors of $X$ with $\ket{\phi_k}$ and the corresponding eigenvalues with $\lambda_k$, we can write any vector $\ket{V}$ (in the same space as the $\ket{\phi_k}$) as a linear combination of $\ket{\phi_k}$ with coefficient $c_k$ as follows.
%
\begin{equation}
\label{eqn:eigcombination}
\ket{V} = \sum_k{c_k\ket{\phi_k}}
\end{equation}
Then, applying QPE to state $\ket{V}$ yields measurements of each eigenvalue $\lambda_k$ with probability $\abs{c_k}^2$, while making this state collapse to the corresponding $\ket{\phi_k}$.
If the matrix $X$ is available in quantum form, i.e., a quantum state with density matrix $X$, using $X$ itself as initial state yields the state 
\begin{equation}
\label{eqn:qpcastate}
\sum_k{\lambda_k\ket{\phi_k}\bra{\phi_k}\otimes\ket{\lambda_k}\bra{\lambda_k}}
\end{equation}
Indeed, a density matrix that represents a covariance matrix (normalized w.r.t. its trace) corresponds in general to a mixed quantum state, that by definition can be decomposed into a probabilistic mixture of its eigenstates (\textit{i.e.} eigenvectors) with weights given by the eigenvalues.
Then, if $X$ is well approximated by a low rank matrix, meaning that few eigenvectors can explain most of the variance in the original data\footnote{to see this, consider that the trace of $X$ is dominated by a few large eigenvalues}, this state will be dominated by few eigenvectors with largest eigenvalues and sampling from it allows to efficiently determine the low rank principal subspace, \cite{Lloyd_2014}.

While this is certainly the preferable solution, if a state $X$ is not available the procedure can still be applied to random initial states, so that in Eq. (\ref{eqn:eigcombination}) the probability that there exists a $c_k=0$ is zero and every eigenvector will have some contribution. Any eigenvalue-eigenvector couple can be obtained in a polynomial number of trials as long as the corresponding $c_k$ is not exponentially small w.r.t. the size of the problem, \cite{Abrams_1999}.

This is a critical aspect because once the circuit is executed to produce a state which encodes information on the eigendecomposition of the matrix, we still need to extract this quantum information into classical form. So far when we referred to states encoding eigenvectors, that was by way of amplitude encoding, i.e., given a vector $\mathbf{x} = \begin{pmatrix} x_0 & x_1 & \hdots & x_n \end{pmatrix}^\top$ it is encoded using $\log_2{n}$ qubits so that 
\begin{equation}
\label{eqn:amplencoding}
\ket{\textbf{x}} = x_0\ket{0\hdots00} + x_1\ket{0\hdots01} + \hdots + x_n\ket{1\hdots11}
\end{equation}
Since in the final state of the algorithm the eigenvalues and the eigenvectors are entangled, when a $\lambda_k$ is measured the corresponding $\ket{\phi_k}$ can be sampled; reconstructing this state corresponds to determining the vector's components.

The process of reconstructing a quantum state through measurements is called Quantum State Tomography, and is in general computationally intensive: $\mathcal{O}(4^b)$ different observables must be measured for a $b$-qubit system, and assigning a physically valid density matrix from the gathered data requires an optimization procedure, as discussed in  \cite{lecnotesphys}. 
However, as \cite{Aaronson_2007} points out, in many cases one could be interested in predicting only certain properties of a quantum state without fully characterizing it, which can be done using only a polynomial number of samples.
At any rate, the problem of efficiently "reading" quantum information encoded in amplitudes, which is common in QML applications, is still not fully resolved.

Another crucial part of the algorithm is the implementation of unitary  $U=e^{iX}$, as required to perform the QPE. The problem of finding a circuit that approximates such unitary operator for a given $2^n\times2^n$ hermitian matrix $X$ has also been widely discussed in the literature under the name of Hamiltonian Simulation \eli{Here we would need a reference}.

A method for the exponentiation of non-sparse $d$-dimensional Hamiltonians in time $\mathcal{O}(\log{d})$ is also proposed in \cite{Lloyd_2014}, and is compared to methods based on the higher order Suzuki-Trotter expansion that runs in $\mathcal{O}(d\log{d})$. However, this technique requires once again that many copies of quantum state $X$ are given, and is most effective when some of the eigenvalues of $X$ are large. It was proven that this method is optimal in terms of sample complexity \cite{Kimmel_2017}, meaning it shows the best bound on the approximation error for a given number of copies of $X$.\\
Other methods using different and more convenient input models have been proposed, like the one of \cite{wang2018quantum} based on a QRAM storing the entries of the Hamiltonian and with time complexity $\mathcal{\Tilde{O}}(\sqrt{d})$.

Finally, it should be pointed out that different algorithms for QPE have been developed, some that are faster than the one presented here which is based on the IQFT. Referring to the circuit of Fig.\ref{fig:qpecirc}, the gate (time) complexity is $\mathcal{O}(m^2)$ for the IQFT plus $m$ Hadamards and calls to powers of $c-U$. If $c-U^k$ is implemented as $k$ calls to $c-U$, then the complexity is $\mathcal{O}(m^2 + 2^m)$. In this case, the complexity scales as the accuracy of the result, which is linear in $M=2^m$.

\begin{figure*}[t]
    \makebox[\textwidth][c]{
    \begin{tikzpicture}
    \node[scale=0.8] {
        \begin{quantikz}[row sep=0.1cm, column sep=0.1cm]
            \lstick{$\ket{0}$} \gategroup[wires=7,steps=2,style={inner xsep=-20pt, inner ysep =10pt, draw=red, xshift=-15}]{Phase Kickback}& \gate{H} & \qw & \qw & \qw & \ctrl{5} & \qw\gategroup[wires=7,steps=1,style={inner xsep=10pt,inner ysep =10pt, draw=blue,inner xsep=135pt,xshift=102}]{IQFT}& \gate{H} & \ctrl{1} & \qw & \ctrl{3} & \qw & \qw & \ctrl{4} & \qw & \qw & \qw & \qw & \rstick{$\ket{x_m}$}\\
            \lstick{$\ket{0}$} & \gate{H} & \qw & \qw & \ctrl{4} & \qw & \qw & \qw & \gate{R_2^{-1}} & \gate{H} & \qw & \ctrl{2} &\qw & \qw & \ctrl{3} & \qw & \qw & \qw & \rstick{$\ket{x_{m-1}}$}\\
            \vdots & & & & & & & & & & & & & & & & & & \vdots\\
            \lstick{$\ket{0}$}  & \gate{H} & \qw & \ctrl{2} & \qw & \qw & \qw & \qw &\qw &\qw &\gate{R_{m-1}^{-1}} &\gate{R_{m-2}^{-1}} &\gate{H} &\qw &\qw & \ctrl{1} &\qw & \qw & \rstick{$\ket{x_2}$}\\
            \lstick{$\ket{0}$} & \gate{H} & \ctrl{1} & \qw & \qw & \qw & \qw & \qw & \qw &\qw &\qw &\qw &\qw & \gate{R_{m}^{-1}} &\gate{R_{m-1}^{-1}} &\gate{R_{2}^{-1}} &\gate{H} &\qw & \rstick{$\ket{x_1}$}\\
            \lstick{$\ket{\psi}$} & \qw & \gate{c-U^{2^0}} & \gate{c-U^{2^1}} & \gate{c-U^{2^{m-2}}} & \gate{c-U^{2^{m-1}}} & \qw & \qw & \qw & \qw & \qw & \qw & \qw & \qw & \qw & \qw & \qw & \qw & \rstick{$\ket{\psi}$}\\
            \end{quantikz}
    };\end{tikzpicture}
    }
    \caption[.]{
        \begin{minipage}{\linewidth}Circuit for QPE adopted in our implementation of the algorithm.
        \(R_k^{-1} = \begin{pmatrix} 1 & 0 \\ 0 & e^{\frac{2\pi i}{2^k}} \end{pmatrix}\)
        \end{minipage}
        }
    \label{fig:qpecirc}
\end{figure*}

\section{Approach}
\label{sec:impl}
An implementation of the algorithm was realized in Python using Qiskit, and is available in the repository for this work \cite{QPCArepo}.\\
Phase estimation was implemented using the circuit described in Section~\ref{sec:pre}, and a roundoff parameter was included to consider reduced precision approximations and increase the probability of the correct outcome as explained.
For the realization of the $c-U$ gates instead, the complex matter of Hamiltonian simulation was here avoided, and matrix exponentiation was performed simply using the \texttt{expm} function of NumPy \cite{oliphant2006guide}.

For what regards the tomography of eigenstates, in our implementation a simple method was adopted that only measures the system in the computational Z basis, to estimate the amplitudes absolute values, and in the ``one-X basis", i.e., one qubit at a time in the X basis, to estimate the relative phases. With data gathered in these $b+1$ settings (for $b$ qubits encoding the states), $\mathcal{O}(2^b)$  square root and comparison operations are then performed to obtain an approximation of the state vector, linearly in the Hilbert space dimension.
We are implicitly using the fact that the states will only have real amplitudes (in an ideal execution), as by the described amplitude encoding technique.

In order to build the circuit, an initial state must be provided as input to the algorithm, and the original procedure of \cite{Lloyd_2014} requires, as discussed, that the input matrix itself be encoded into a quantum state for this purpose. In our case however, we assume that we don't have access to such state, and that it's not the result of previous quantum computations. Instead, we approximate it iteratively starting from a mixture of random orthogonal pure states.

In order to do this for an $n$-dimensional matrix, we consider a random set of $n$ orthogonal vectors and the mixed state described by these vectors, each with same probability $1/n$. We can use this as initial state and expect to measure all eigenvalues and eigenvectors, but we have no guarantees that the final state will be dominated by the principal components of our matrix. Rather, we expect a uniform distribution, as each pure state of the initial uniform mixture is more likely to collapse to different eigenstates. Still, taking few samples from this final state can be enough to find rough estimates of the true eigenvalues and eigenvectors, and effectively obtain an estimate of the matrix in its eigendecomposition. A corresponding mixed state can then be prepared, using the eigenvectors estimates as pure states (generally non-orthogonal, as they have been reconstructed from measurements) weighted by the estimated eigenvalues. Applying the algorithm again using this new state now yields a final state that approximates the one in Eq. (\ref{eqn:qpcastate}), and this is indeed the case since we are using an initial state which approximates the original matrix. 
Thus, sampling from this final state allows to more efficiently reconstruct the principal components, which is our goal for performing QPCA.
Then as we collect more information on the eigendecomposition of our matrix we can iterate the procedure again: the state converges towards the representation of the matrix itself, progressively cutting out the smaller eigenvalues from the output.

At any iteration, the true eigenvalues can in principle be identified just by taking the $N$ most observed values, under the assumption that the number of precision qubits is sufficient to distinguish all of them.
However, this assumption often doesn't hold when the rank of the matrix is small and most of the eigenvalues are grouped together close to zero, indistinguishable from each other.
In this case, although we are not interested in these low eigenvalues, just looking for the $N$ most observed values can lead to picking up fictitious eigenvalues in the high part of the spectrum, which makes the result meaningless. To avoid this, we can consider the most observed values in order, while keeping a partial sum $S$ so that when an eigenvalue $\lambda$ is selected, $\lambda - \frac{1}{2^m} $ is added to $S$, where we remove half quantization step size to account for rounding. Then, at each step only values smaller than $1 - S$ can be selected. 
This simple trick constrains the estimated eigenvalues to sum up to one, which we know a priori to be true.
% \todo[color=green, inline]{(?) Following part could be moved to discussion/conclusion. Leave here only the description of the approach and save the "analysis" and interpretation for later}

Note that obtaining accurate results still requires that accurate state tomography methods be employed and that a suitable number of precision qubits be used for phase estimation.
The advantage that we gain is in the fact that we sample from a state that gives us each eigenvector loosely proportionally to the corresponding eigenvalue. Exact proportionality is achieved in principle when the initial state represents exactly the covariance matrix and the obtained state is described by Eq. \eqref{eqn:qpcastate}, but even then we would have to take into account at the very least the error probability of the QPE.
This proportionality is a useful property if we are interested only in the largest eigenvalues and corresponding eigenvectors, as is the case for PCA, because it allows to devote less computational effort to the reconstruction of components that are not relevant.
The idea behind the adopted procedure is to reach this situation from a random initialization by just probing the output state of the phase estimation circuit, and converging to it as more accurate measurements cause the estimates to improve.
In this sense, the procedure can be considered as a scheduling protocol for efficient principal subspace tomography, although that of iterative approximation of a mixed state is a nice theoretical interpretation.

\section{Experiments}
\label{sec:exp}
% \todo[inline]{add here everything that you have done but wasn't initially included in your report, including a few diagrams in your presentation. Results should have one subsection for simulation + one for experimental results + one section for discussing the results.}
A demonstration of how our implementation of the algorithm and the proposed protocol work is provided in the form of a Jupyter Notebook at \cite{QPCAnotebook}.

We first tested the algorithm in Qiskit's QASM simulator, using a $4\times4$ synthetic covariance matrix. The matrix is represented by the eigenedecomposition
\begin{center}
\begin{tabular}{cl}
$\lambda_1 = 0.511$     & $\phi_1 = (-0.597, -0.306, -0.312, -0.670)$\\
$\lambda_2 = 0.299$     & $\phi_2 = (0.442, -0.647, -0.594, 0.185)$\\
$\lambda_3 = 0.147$     & $\phi_3 = (-0.442, 0.410, -0.619, 0.503)$\\
$\lambda_4 = 0.043$     & $\phi_4 = (0.503, 0.566, -0.403, -0.515)$\\
\end{tabular}
\end{center}
We generated a random set of $4$ orthonormal vectors, which were taken to be orthogonal pure states, and considered the mixture of these states, each with probability $1/4$. 
The first step of the protocol is to apply phase estimation to this mixed state. 
Since the framework provided by Qiskit does not support a straightforward way to prepare mixed quantum states, we simulated this behaviour by running exactly the same number of shots on each of the pure states individually, which is the expected outcome for a state constructed in this way.
Since the goal at this stage is only to probe the output state, we perform only few shots. In this case, $10$ shots were performed for each pure state. In our implementation, this number is repeated for every measurement basis, meaning $30$ effective shots for a state encoded over $2$ qubits.
The resulting measurements for all initial states are then merged together, forming a single data structure that contains samples of the eigenstates grouped by corresponding eigenvalues.
All of this was done using $8$ qubits for the eigenvalues estimation and then rounding off to $6$-bits approximations.
The results are illustrated in the histogram in Fig. \ref{fig:4x4hist1}, which shows that all the eigenvalues appeared in output almost uniformly.
In this case, approximations for all true eigenvalues are clearly identifiable, as the four most observed values are
\begin{center}
\begin{tabular}{c}
$\tilde{\lambda_1} = 0.515625$ \\
$\tilde{\lambda_2} = 0.296875$ \\
$\tilde{\lambda_3} = 0.156250$ \\
$\tilde{\lambda_4} = 0.046875$ \\
\end{tabular}
\end{center}
The estimates of the eigenstates they are entangled with give us an approximation of the corresponding eigenvectors.
The procedure was then repeated for a second iteration, this time using each eigenvector estimate as pure state in a probabilistic mixture with weights given by the estimated eigenvalues, normalized to sum up to $1$.
Again, this was simulated by running $\tilde{\lambda_k}\cdot N\cdot C$ shots on each state individually, where $C$ is a constant that determines the number of shots over all states as $N\cdot C$. From previously being equal to $10$, this value was increased to $1000$ as our approximation of the goal state was now improved, and measurements from all states were again merged together and with ones from the previous iteration.\\
Indeed, the histogram depicted in Fig. \ref{fig:4x4hist2} shows that in the final state which was obtained, the eigenvectors corresponding to larger eigenvalues are dominant, and were overall sampled more often.\\
In order to assess the quality of our estimates, we can calculate the relative error on a low-rank reconstruction of the original matrix using this quantum eigendecomposition with respect to the one obtained using the exact eigendecomposition. Calling $\Lambda_K$ the diagonal matrix whose elements are the $K$ largest eigenvalues, and $V_K$ the matrix whose columns are the corresponding eigenvectors, we can compute the two low-rank reconstructions as $V_K\Lambda_K V_K^\top$ using both the estimated components and the classically computed ones, and calculate the Frobenius norm of their difference. Then, dividing by the norm of the exact reconstruction we obtain the relative error. For instance, by considering only the first $K=2$ principal components we got an error of approximately $0.019$.
This quantity accounts for both the approximation error of eigenvalues and the inaccuracy of eigenstates tomography, which can be improved respectively by using larger quantum registers (for holding the eigenvalues) and more accurate tomography methods, as discussed. %remove "as discussed" if that part gets moved to later

\begin{figure*}[t]
\centering
    \begin{subfigure}[b]{.75\textwidth}
    \includegraphics[width=\linewidth]{4x4_sim_hist1.png}
    \subcaption{}
    \label{fig:4x4hist1}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{.75\textwidth}
    \includegraphics[width=\linewidth]{4x4_sim_hist2.png}
    \subcaption{}
    \label{fig:4x4hist2}
    \end{subfigure}
    \caption{Histogram of measured eigenvalues in the first (a) and second (b) iteration for the $4\times4$ covariance matrix, executed on the simulator.}
\end{figure*}

The algorithm was then tested on IBM's 5-qubit quantum device Athens. For this purpose, a $2\times2$ covariance matrix  was used whose eigendecomposition is
\begin{center}
\begin{tabular}{cl}
$\lambda_1 = 0.713$     &  $\phi_1 = (-0.439, -0.898)$\\
$\lambda_2 = 0.287$     & $\phi_2 = (-0.898, 0.439)$
\end{tabular}
\end{center}
This time only $2$ qubits were used for the eigenvalues estimation, to which a single qubit is added for encoding the eigenstates.
Following what was done on the simulator, we considered a mixed state consisting of two random orthogonal states, and executed $10$ shots (per measurement basis) on each of those.\\
Fig. \ref{fig:2x2hist1} shows how the eigenvalues were revealed in this first iteration. The corresponding eigenvectors were estimated as
\begin{center}
\begin{tabular}{cl}
$\tilde{\lambda_1} = 0.75$     &  $\tilde{\phi_1} = (0.707, 0.707)$\\
$\tilde{\lambda_2} = 0.25$     & $\tilde{\phi_2} = (0.894, -0.450)$
\end{tabular}
\end{center}
Note that we found inverted axes with respect to the ones computed classically. Of course we would have no way to tell them apart in the quantum algorithm, since they differ only by a global phase shift, but this is also not a problem at all for PCA because we still get the directions of maximum explained variance.\\
These states were used for a second iteration, weighting $N\cdot C = 2\cdot1000$ shots proportionally to their eigenvalues in order to simulate the application on state \mbox{$\tilde{\lambda_1}\ket{\tilde{\phi_1}}\bra{\tilde{\phi_1}} + \tilde{\lambda_2}\ket{\tilde{\phi_2}}\bra{\tilde{\phi_2}}$}.\\ 
We thus obtained the measurements in Fig. \ref{fig:2x2hist2}, which confirm that the state was converging towards the representation of the matrix. The final approximation of the eigendecomposition was
\begin{center}
\begin{tabular}{cl}
$\tilde{\lambda_1} = 0.75$     &  $\tilde{\phi_1} = (0.492, 0.866)$\\
$\tilde{\lambda_2} = 0.25$     & $\tilde{\phi_2} = (0.848, -0.529)$
\end{tabular}
\end{center}
Although in this case the approximation for $\phi_2$ got slightly worse, the largest principal component $\phi_1$ improved as expected, and considering the rank-one reconstruction we get a relative error of $0.079$.

Lastly, we ran the algorithm on the real quantum processor, applied to the same $4\times4$ covariance matrix that was tested on the simulator.
In this case, hoping to identify anything more than the first principal component would require that the number of precision qubits be increased to at least 3. However, in order to keep the complexity of the circuit and the decoherence effect lower, we again used 2 qubits for the eigenvalue estimation, which could ideally give us sensible results for the approximation of $\lambda_1 = 0.511$ and its eigenvector. After two iterations of the algorithm, in the same fashion of the previous experiments, we obtained the measurements shown in Fig. \ref{fig:4x4hist_real}, and an eigenvector corresponding to $\tilde{\lambda_1} = 0.5$ estimated as $\tilde{\phi_1} = (0.486, 0.520, -0.477, -0.515)$. These appear to be random results from which no useful information can be extracted, signalling that decoherence-induced errors are already too high for a problem of this size.

\begin{figure}[t]
\centering
    \begin{subfigure}[b]{\columnwidth}
    \includegraphics[width=\linewidth]{2x2_hist_real1.png}
    \subcaption{}
    \label{fig:2x2hist1}
    \end{subfigure}
    \begin{subfigure}[b]{\columnwidth}
    \includegraphics[width=\linewidth]{2x2_hist_real2.png}
    \subcaption{}
    \label{fig:2x2hist2}
    \end{subfigure}
    \caption{Histogram of measured eigenvalues in the first (a) and second (b) iteration for the $2\times2$ covariance matrix, executed on the real quantum processor.}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{4x4_hist_real.png}
\caption{Values measured after the second iteration for a $4\times4$ covariance matrix executed on the real quantum processor.}
\label{fig:4x4hist_real}
\end{figure}

\section{Discussions}
\label{sec:disc}
It's easy to see that in the approach followed by \cite{martin2019pricing}, after the algorithm is applied to a random initialization, subsequent iterations executed on the estimated largest eigenvector correspond to applications on a rank-one reconstruction of the original matrix, up to a scale factor.
A natural generalization then, which we have shown here, is to consider multiple eigenvectors estimates, together with the corresponding eigenvalues, to produce a state which better approximates the original matrix, and iterate on this instead.
While in this way we consider a higher rank reconstruction, which might seem counterproductive given that our goal is to obtain a low rank approximation, we do so in the way dictated by the original algorithm.
Indeed, when dealing with low rank matrices, components associated to very small or zero eigenvalues will give little to no contribution to the state that gets formed.
On the other hand, no a priori decision is forced on the number of components to retain, which is problem dependent and can otherwise undermine the generality of the method. 

We have in this sense suggested an interpretation which we believe could help to highlight the connection between this iterative procedure and the theoretical basis of the algorithm, as the approximation of the state encoding the covariance matrix obtained starting from a random mixed state.
However, this amounts to the application of phase estimation on each of the estimated eigenstates proportionally, in expectation, to their probabilities, which is how we have implemented it in Qiskit.
This is fundamentally no different from the case in which a single eigenstate is estimated, except that if the experiment is repeated for a fixed number of times, i.e., the number of shots we execute for our algorithm, this quantity is distributed among the different eigenstates in order to obtain, with high probability, samples of the largest ones.

Then, as discussed, decoding information stored in the amplitudes of a quantum state from samples is not an easy task, and solutions to do it in a way that preserves the potential speed-up achieved by the algorithm are still being researched.
The method that we have adopted is rather naive and could be substituted by more accurate and efficient ones, which could involve measurements in additional different directions.
An efficient read-out protocol for this kind of application is proposed in \cite{zhang2020efficient}, and as the authors suggest it could be extended precisely to decode the eigenstates when performing QPCA. 
In any case, this modification could be done ideally without changing the core of the algorithm, which is based on the general fact that repeated measurements of a quantum state improve the accuracy of its estimation.

As we mentioned, this procedure can be thought of as a dynamic scheduling protocol for principal subspace tomography, in that we restrict our attention to the relevant components as we collect data on them.
In principle, the initial state of the circuit could be updated  after every execution, when a measurement is taken and the estimate of an eigenstate is improved, with the corresponding eigenvalue being revealed.
However, we also have to take into account the cost of preparing the desired states, which causes a non-negligible overhead, so that a trade-off must be balanced. 

In our experiments we have shown some numerical examples of the algorithm, demonstrating how the procedure is ideally executed on the simulator.
On the real quantum computer we were able to solve a small instance of the problem, but for a $4\times4$ matrix the generated circuit is already too large and errors caused by quantum decoherence and noise led to unusable results.
This is clear in Fig. \ref{fig:4x4hist_real}, which shows that the output of phase estimation doesn't give any information on the true eigenspectrum of the input matrix.
Despite using a small number of precision qubits, the depth of the circuit is largely increased in this case due to the decomposition of the larger multi-qubit controlled gates into two-qubit gates, that happens at transpilation time.
In this regard, we encountered similar problems to those observed by \cite{DBLP:conf/qce/PiroAN20} when moving from the simulator to a real quantum processor.
% (?)
% this is due to resources limits (quantity and quality) but algo is ready in terms of framework

\section{Conclusions}
\label{sec:conc}
We have discussed the original QPCA algorithm, and we have presented an implementation that is closer to a generalized version of \cite{martin2019pricing}'s modified procedure, who were, to our knowledge, the first to show practical experiments on this matter.

We have provided Qiskit code for the algorithm described in this work, that was used for performing our experiments, as we believe that with the rapidly growing interest in quantum computing there is also a demand for tangible demonstrations of its applications.

In discussing this algorithm we have briefly reviewed some critical issues that apply to many other problems in quantum computing and QML, that need to be solved before practical real-world problems can be tackled with a quantum advantage in the NISQ era.


%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}


% Authors must disclose all relationships or interests that 
% could have direct or potential influence or impart bias on 
% the work: 
%
% \section*{Conflict of interest}
%
% The authors declare that they have no conflict of interest.


% BibTeX users please use one of
\bibliographystyle{spbasic} % basic style, author-year citations
% \bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{references}   % name your BibTeX data base
% \printbibliography

\end{document}
